---
title: "Combined"
author: "Bo Li U24425931"
date: "3/1/2021"
output:
  pdf_document: default
  html_document: default
---
# 1. First thing to add: cross-validation plot
```{r}
library(data.table)
library(ggplot2)
library(ggthemes)
library(glmnet)
theme_set(theme_bw())
library(MASS)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(e1071)
library(tree)
library(ISLR)
library(party)
library(tidymodels)
library(caTools)
```
data <- fread("C:/Users/boli0/Downloads/train.csv")
str(data)
data$price_range <- as.factor(data$price_range)

set.seed(810)
split = sample.split(data$price_range, SplitRatio = 0.7)
data_train = subset(data, split == TRUE)
data_test = subset(data, split == FALSE)
y_test <- data_test[,price_range]

# grow tree
fit <- rpart(price_range ~., method = "class", data = data_train)
```{r}
printcp(fit)
```
```{r}
plotcp(fit) #visualize cross-validation results
```

# 2. Second thing to add: Feature importance scatterplot
rFM.data <- randomForest(price_range ~., data = data_train, mytry = sqrt(20), importance = TRUE, proximity = TRUE)
```{r}
# Random Forest Feature Importance Barplot
barplot(rFM.data$importance[,2], main = "Feature Importance Barplot")
```
```{r}
# Random Forest Feature Importance ScatterPlot
varImpPlot(rFM.data, sort = TRUE, n.var = nrow(rFM.data$importance), main = "Feature Importance ScatterPlot")
```
# 3. Third thing to add: boosting/bagging decision tree
library(adabag)

data.adaboost <- boosting(price_range ~., data = data_train, mfinal = 10, control = rpart.control(maxdepth = 1))
data.adaboost

data.predboosting <- predict.boosting(data.adaboost, newdata = data_test)
data.predboosting

data.boostcv <- boosting.cv(price_range ~., v = 10, data = data_train, mfinal = 10, control = rpart.control(maxdepth = 1))
data.boostcv

```{r}
barplot(data.adaboost$imp[order(data.adaboost$imp, decreasing = TRUE)], ylim = c(0, 100), main = "Variables Relative Importance", col = "lightblue")
```

data.bagging <- bagging(price_range ~., data = data_train, mfinal = 10, control = rpart.control(maxdepth = 1))
data.bagging

data.predbagging <- predict.bagging(data.bagging, newdata = data_test)
data.predbagging

data.baggingcv <- bagging.cv(price_range ~., v = 10, data = data_train, mfinal = 10, control = rpart.control(maxdepth = 1))
data.baggingcv

data.bagging.margins <- margins(data.bagging, data_train)
data.bagging.margins

data.bagging.predmargins <- margins(data.predbagging, data_test)
data.bagging.predmargins

```{r}
margins.test <- data.bagging.predmargins[[1]]
margins.train <- data.bagging.margins[[1]]
plot(sort(margins.train), (1:length(margins.train)) / length(margins.train), type = "l", xlim = c(-1,1), main = "Margin Cumulative Distribution Graph", xlab = "m", ylab = "% observations", col = "blue3", lwd = 2)
abline(v = 0, col = "red", lty = 2, lwd = 2)
lines(sort(margins.test), (1:length(margins.test)) / length(margins.test), type = "l", cex = 0.5, col = "green", lwd = 2)
legend("topleft", c("test","train"), col = c("green", "blue"), lty = 1, lwd = 2)
```
```{r}
evol.test <- errorevol(data.adaboost, data_test)
evol.train <- errorevol(data.adaboost, data_train)
plot(evol.test$error, type = "l", ylim = c(0, 1),
main = "Boosting Error Versus Number Of Trees", xlab = "Iterations",
ylab = "Error", col = "red", lwd = 2)
lines(evol.train$error, cex = .5, col = "blue", lty = 2, lwd = 2)
legend("topleft", c("test", "train"), col = c("red", "blue"), lty = 1:2, lwd = 2)
```